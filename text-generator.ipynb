{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Обучение языковой модели с помощью LSTM\n\nЦель: построить генеративную языковую модель, которая по началу/теме предложения сможет генерировать осмысленный текст.\nОсновной задачей является экспериментирование с разными схемами моделей и параметрами, грамотная оценка качества модели через перплексию.\n\n---","metadata":{}},{"cell_type":"markdown","source":"## 0. Зависимости.\nСтандартные библиотеки для работы с текстом. Настройка MLflow для логирования экспериментов.","metadata":{}},{"cell_type":"code","source":"!pip install datasets\n!pip install mlflow","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:11:56.818288Z","iopub.execute_input":"2023-06-06T14:11:56.818701Z","iopub.status.idle":"2023-06-06T14:12:20.650463Z","shell.execute_reply.started":"2023-06-06T14:11:56.818667Z","shell.execute_reply":"2023-06-06T14:12:20.649179Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (10.0.1)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.28.2)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.64.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.14.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.4.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: mlflow in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (8.1.3)\nRequirement already satisfied: cloudpickle<3 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.2.1)\nRequirement already satisfied: databricks-cli<1,>=0.8.7 in /opt/conda/lib/python3.10/site-packages (from mlflow) (0.17.7)\nRequirement already satisfied: entrypoints<1 in /opt/conda/lib/python3.10/site-packages (from mlflow) (0.4)\nRequirement already satisfied: gitpython<4,>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.1.31)\nRequirement already satisfied: pyyaml<7,>=5.1 in /opt/conda/lib/python3.10/site-packages (from mlflow) (5.4.1)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.20.3)\nRequirement already satisfied: pytz<2024 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2023.3)\nRequirement already satisfied: requests<3,>=2.17.3 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.28.2)\nRequirement already satisfied: packaging<24 in /opt/conda/lib/python3.10/site-packages (from mlflow) (21.3)\nRequirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (5.2.0)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (0.4.4)\nRequirement already satisfied: alembic!=1.10.0,<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.11.1)\nRequirement already satisfied: docker<7,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (6.1.1)\nRequirement already satisfied: Flask<3 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.3.2)\nRequirement already satisfied: numpy<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.23.5)\nRequirement already satisfied: scipy<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.10.1)\nRequirement already satisfied: pandas<3 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.5.3)\nRequirement already satisfied: querystring-parser<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.2.4)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.0.12)\nRequirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.2.2)\nRequirement already satisfied: pyarrow<13,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (10.0.1)\nRequirement already satisfied: markdown<4,>=3.3 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.4.3)\nRequirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.6.3)\nRequirement already satisfied: gunicorn<21 in /opt/conda/lib/python3.10/site-packages (from mlflow) (20.1.0)\nRequirement already satisfied: Jinja2<4,>=2.11 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.1.2)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (1.2.4)\nRequirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (4.5.0)\nRequirement already satisfied: pyjwt>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (2.6.0)\nRequirement already satisfied: oauthlib>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (3.2.2)\nRequirement already satisfied: tabulate>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (0.9.0)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.16.0)\nRequirement already satisfied: urllib3<2.0.0,>=1.26.7 in /opt/conda/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.26.15)\nRequirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from docker<7,>=4.0.0->mlflow) (1.5.1)\nRequirement already satisfied: Werkzeug>=2.3.3 in /opt/conda/lib/python3.10/site-packages (from Flask<3->mlflow) (2.3.4)\nRequirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask<3->mlflow) (2.1.2)\nRequirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from Flask<3->mlflow) (1.6.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython<4,>=2.1.0->mlflow) (4.0.10)\nRequirement already satisfied: setuptools>=3.0 in /opt/conda/lib/python3.10/site-packages (from gunicorn<21->mlflow) (59.8.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow) (3.15.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow) (2.1.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.0.7)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (4.39.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (9.5.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (2.8.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (2023.5.7)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (3.1.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (2.0.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow) (5.0.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tqdm.auto import tqdm\nfrom datasets import load_dataset\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom sklearn.model_selection import train_test_split\nimport nltk\n\nfrom collections import Counter\nfrom typing import List\nimport mlflow\nimport seaborn\n\n\n#Секрет - адрес mlflow-сервера\nfrom kaggle_secrets import UserSecretsClient\n\nsecret_value = UserSecretsClient().get_secret(\"mlflow_url\")\nmlflow.set_tracking_uri(secret_value)\nseaborn.set(palette='summer')\n","metadata":{"_uuid":"9aa01ffe-94e6-4ccc-9717-848205cad993","_cell_guid":"224616fc-d042-4dec-a4ba-edffc187d141","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-06T14:12:20.654303Z","iopub.execute_input":"2023-06-06T14:12:20.654613Z","iopub.status.idle":"2023-06-06T14:12:20.852932Z","shell.execute_reply.started":"2023-06-06T14:12:20.654581Z","shell.execute_reply":"2023-06-06T14:12:20.852099Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:12:20.854857Z","iopub.execute_input":"2023-06-06T14:12:20.855423Z","iopub.status.idle":"2023-06-06T14:12:20.864688Z","shell.execute_reply.started":"2023-06-06T14:12:20.855390Z","shell.execute_reply":"2023-06-06T14:12:20.863524Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"## 1. Подготовка данных.\nИспользуется датасет imdb. В нем хранятся отзывы о фильмах с сайта imdb.","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset('imdb')","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:12:20.868258Z","iopub.execute_input":"2023-06-06T14:12:20.868618Z","iopub.status.idle":"2023-06-06T14:12:21.185179Z","shell.execute_reply.started":"2023-06-06T14:12:20.868568Z","shell.execute_reply":"2023-06-06T14:12:21.184124Z"},"trusted":true},"execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37a075345ecb4e2483ba2fe2c577874f"}},"metadata":{}}]},{"cell_type":"markdown","source":"## 2. Препроцессинг данных и создание словаря\nДалее необходмо получить словарь или же просто set строк. Что необходимо сделать:\n\n- Разделить отдельные тренировочные примеры на отдельные предложения с помощью функции sent_tokenize из бибилиотеки nltk. Каждое отдельное предложение будет одним тренировочным примером.\n- Оставить только те предложения, в которых меньше word_threshold слов.\n- Посчитать частоту вхождения каждого слова в оставшихся предложениях. Для деления предложения на отдельные слова удобно использовать функцию word_tokenize.\n- Создать объект vocab класса set, положить в него служебные токены ```'<unk>', '<bos>', '<eos>', '<pad>'``` и vocab_size самых частовстречающихся слов.","metadata":{}},{"cell_type":"code","source":"# Первые два пункта\ndef filter_on_length(sentence: str, threshold=32):\n    # Фильтруем по длине, отбрасывая длинные.\n    word_count = len(word_tokenize(sentence))\n    if word_count > threshold:\n        return False\n    return True\n\ndef get_sentences(data: Dataset, threshold=32) ->list:\n    # Преобразуем датасет в список с предложениями\n    sentences = []\n    for review in tqdm(data[\"text\"]):\n        sentences +=[x.lower() for x in sent_tokenize(review) if filter_on_length(x, threshold)]\n    return sentences","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:12:21.186442Z","iopub.execute_input":"2023-06-06T14:12:21.187166Z","iopub.status.idle":"2023-06-06T14:12:21.195805Z","shell.execute_reply.started":"2023-06-06T14:12:21.187132Z","shell.execute_reply":"2023-06-06T14:12:21.194526Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"sentence_list = get_sentences(dataset[\"train\"])\nprint(len(sentence_list))","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:12:21.197893Z","iopub.execute_input":"2023-06-06T14:12:21.198340Z","iopub.status.idle":"2023-06-06T14:13:51.940937Z","shell.execute_reply.started":"2023-06-06T14:12:21.198307Z","shell.execute_reply":"2023-06-06T14:13:51.939871Z"},"trusted":true},"execution_count":42,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ef91cf37bd8424f9b873d69e00bc783"}},"metadata":{}},{"name":"stdout","text":"202606\n","output_type":"stream"}]},{"cell_type":"code","source":"# Создание словаря\nvocab = set(['<unk>', '<bos>', '<eos>', '<pad>'])\nvocab_size = 40000\n\ndef count_words(sentences, size):\n    word_dict = {}\n    for sentence in tqdm(sentences):\n        for word in Counter(word_tokenize(sentence)):\n            if word in word_dict:\n                word_dict[word] += 1\n            else:\n                word_dict[word] = 1\n    word_dict = [key for key, _ in sorted(word_dict.items(), key=lambda item: item[1], reverse=True)]\n    return word_dict[:size]","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:13:51.942499Z","iopub.execute_input":"2023-06-06T14:13:51.943375Z","iopub.status.idle":"2023-06-06T14:13:51.955599Z","shell.execute_reply.started":"2023-06-06T14:13:51.943339Z","shell.execute_reply":"2023-06-06T14:13:51.954705Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"vocab.update(count_words(sentence_list, 40000))\nassert '<unk>' in vocab\nassert '<bos>' in vocab \nassert '<eos>' in vocab \nassert '<pad>' in vocab\nassert len(vocab) == vocab_size + 4\nprint(len(vocab))","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:13:51.957205Z","iopub.execute_input":"2023-06-06T14:13:51.957627Z","iopub.status.idle":"2023-06-06T14:14:37.713599Z","shell.execute_reply.started":"2023-06-06T14:13:51.957593Z","shell.execute_reply":"2023-06-06T14:14:37.712578Z"},"trusted":true},"execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/202606 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d9022db4e9846bdbfc789e42842ec82"}},"metadata":{}},{"name":"stdout","text":"40004\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 3. Dataset и лоадеры.\nДатасет с методом `__getitem__` и затем train- val- loaders","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"word2ind = {word: i for i, word in enumerate(vocab)}\nind2word = {i: word for word, i in word2ind.items()}\n\nclass WordDataset:\n    def __init__(self, sentences):\n        self.data = sentences\n        # Маркеры неизв. слова, начала и конца предложения, паддинга.\n        self.unk_id = word2ind['<unk>']\n        self.bos_id = word2ind['<bos>']\n        self.eos_id = word2ind['<eos>']\n        self.pad_id = word2ind['<pad>']\n\n    def __getitem__(self, idx: int) -> List[int]:\n        # Предложения приведены к нижнему регистру, \n        # но не очищены от стоп-слов и пунктуации.\n        tokenized_sentence = [self.bos_id]\n        tokenized_sentence += [\n            word2ind.get(word, self.unk_id) for word in word_tokenize(self.data[idx])\n            ] \n        tokenized_sentence += [self.eos_id]\n\n        return tokenized_sentence\n\n    def __len__(self) -> int:\n        return len(self.data)\n    \ndef collate_fn_with_padding(\n    input_batch: List[List[int]], pad_id=word2ind['<pad>']) -> torch.Tensor:\n    # Паддинг: нужно выдавать entry одинаковой длины, соотв. заполняем нехват. строки\n    seq_lens = [len(x) for x in input_batch]\n    max_seq_len = max(seq_lens)\n\n    new_batch = []\n    for sequence in input_batch:\n        for _ in range(max_seq_len - len(sequence)):\n            sequence.append(pad_id)\n        new_batch.append(sequence)\n    \n    sequences = torch.LongTensor(new_batch).to(device)\n\n    new_batch = {\n        'input_ids': sequences[:,:-1],\n        'target_ids': sequences[:,1:]\n    }\n\n    return new_batch","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:14:37.714962Z","iopub.execute_input":"2023-06-06T14:14:37.715962Z","iopub.status.idle":"2023-06-06T14:14:37.746602Z","shell.execute_reply.started":"2023-06-06T14:14:37.715921Z","shell.execute_reply":"2023-06-06T14:14:37.745574Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"train_sentences, eval_sentences = train_test_split(sentence_list, test_size=0.2)\neval_sentences, test_sentences = train_test_split(sentence_list, test_size=0.5)\n\ntrain_dataset = WordDataset(train_sentences)\neval_dataset = WordDataset(eval_sentences)\ntest_dataset = WordDataset(test_sentences)\n\nbatch_size = 128\n\ntrain_dataloader = DataLoader(\n    train_dataset, collate_fn=collate_fn_with_padding, batch_size=batch_size)\n\neval_dataloader = DataLoader(\n    eval_dataset, collate_fn=collate_fn_with_padding, batch_size=batch_size)\n\ntest_dataloader = DataLoader(\n    test_dataset, collate_fn=collate_fn_with_padding, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:14:37.751781Z","iopub.execute_input":"2023-06-06T14:14:37.752701Z","iopub.status.idle":"2023-06-06T14:14:37.893626Z","shell.execute_reply.started":"2023-06-06T14:14:37.752666Z","shell.execute_reply":"2023-06-06T14:14:37.892624Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"## 4. Обучение и архитектура модели\n---\nДля обучения и логирования экспериментов понадобится функции train, evaluate, generate.\nTrain - стандартный, с логированием точности и лосса на обучающей и валидационной выборке + построением графиков.\nТочность - из evaluate - перплексия = exp(CrossEntropy).\n\nЭкспериментальная часть подразумевает как перебор гиперпараметров, так и разные конфигурации моделей и нормализации.","metadata":{}},{"cell_type":"code","source":"def fit(train_dataloader, model, criterion, optimizer, epoch):\n    epoch_losses = []\n    model.train()\n    for batch in tqdm(train_dataloader, desc=f'Training epoch {epoch}:'):\n        optimizer.zero_grad()\n        logits = model(batch['input_ids']).flatten(start_dim=0, end_dim=1)\n        loss = criterion(\n            logits, batch['target_ids'].flatten())\n        loss.backward()\n        optimizer.step()\n        epoch_losses.append(loss.item())\n    return epoch_losses\n\ndef evaluate(model, criterion, dataloader) -> float:\n    model.eval()\n    perplexity = []\n    with torch.no_grad():\n        for batch in dataloader:\n            logits = model(batch['input_ids']).flatten(start_dim=0, end_dim=1)\n            loss = criterion(logits, batch['target_ids'].flatten())\n            perplexity.append(torch.exp(loss).item())\n    \n    perplexity = sum(perplexity) / len(perplexity)\n\n    return perplexity\n\ndef train(train_loader, eval_loader, model, criterion, optimizer, epochs=7):\n    losses = []\n    perplexities = []\n    for epoch in range(epochs):\n        epoch_losses = fit(train_loader, model, criterion, optimizer, epoch)\n        mlflow.log_metric(\"train loss\", sum(epoch_losses) / len(epoch_losses))\n        losses.append(sum(epoch_losses) / len(epoch_losses))\n        \n        perp = evaluate(model, criterion, eval_loader)\n        mlflow.log_metric(\"val perplexity\", perp)\n        perplexities.append(perp)\n    return losses, perplexities","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:14:37.894942Z","iopub.execute_input":"2023-06-06T14:14:37.895389Z","iopub.status.idle":"2023-06-06T14:14:37.907053Z","shell.execute_reply.started":"2023-06-06T14:14:37.895355Z","shell.execute_reply":"2023-06-06T14:14:37.905965Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"def plot_results(losses, perplexity):\n    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n    axes[0].plot(np.arange(len(losses)), losses)\n    axes[0].set_title('Losses')\n    axes[0].set_xlabel(\"epoch\")\n    axes[1].plot(np.arange(len(perplexity)), perplexity)\n    axes[1].set_title('Accuracy on val.')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:14:37.908311Z","iopub.execute_input":"2023-06-06T14:14:37.909098Z","iopub.status.idle":"2023-06-06T14:14:37.921592Z","shell.execute_reply.started":"2023-06-06T14:14:37.909037Z","shell.execute_reply":"2023-06-06T14:14:37.920562Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"class WordLM(nn.Module):\n    def __init__(self, \n                 hidden_dim: int, \n                 vocab_size: int,\n                 num_layers=1, \n                 drop_rate=0.1,\n                 batches=False,\n                 rnn_type=\"rnn\"\n                 ):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n        if rnn_type == \"rnn\":\n            self.rnn = nn.RNN(hidden_dim, \n                              hidden_dim, \n                              num_layers=num_layers, \n                              batch_first=True)\n        elif rnn_type == \"gru\":\n            self.rnn = nn.GRU(hidden_dim, \n                              hidden_dim, \n                              num_layers=num_layers, \n                              batch_first=True)\n        else:\n            self.rnn = nn.LSTM(hidden_dim, \n                              hidden_dim, \n                              num_layers=num_layers, \n                              batch_first=True)\n        self.linear = nn.Linear(hidden_dim, hidden_dim)\n        self.projection = nn.Linear(hidden_dim, vocab_size)\n\n        self.non_lin = nn.Tanh()\n        self.dropout = nn.Dropout(p=0.1)\n\n    def forward(self, input_batch) -> torch.Tensor:\n        embeddings = self.embedding(input_batch)  # [batch_size, seq_len, hidden_dim]\n        output, _ = self.rnn(embeddings)  # [batch_size, seq_len, hidden_dim]\n        output = self.dropout(self.linear(self.non_lin(output)))  # [batch_size, seq_len, hidden_dim]\n        projection = self.projection(self.non_lin(output))  # [batch_size, seq_len, vocab_size]\n\n        return projection","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:14:37.922827Z","iopub.execute_input":"2023-06-06T14:14:37.923246Z","iopub.status.idle":"2023-06-06T14:14:37.935024Z","shell.execute_reply.started":"2023-06-06T14:14:37.923212Z","shell.execute_reply":"2023-06-06T14:14:37.934006Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"## 5. Baseline.\n","metadata":{}},{"cell_type":"code","source":"\"\"\"criterion = nn.CrossEntropyLoss(ignore_index=word2ind['<pad>'])\nfirst_model = WordLM(hidden_dim=128, vocab_size=len(vocab)).to(device)\noptimizer = torch.optim.Adam(first_model.parameters())\nlosses, perp = train(train_dataloader, eval_dataloader, first_model, criterion, optimizer, epochs=10)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:14:37.936828Z","iopub.execute_input":"2023-06-06T14:14:37.937621Z","iopub.status.idle":"2023-06-06T14:14:37.953563Z","shell.execute_reply.started":"2023-06-06T14:14:37.937595Z","shell.execute_reply":"2023-06-06T14:14:37.952558Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"\"criterion = nn.CrossEntropyLoss(ignore_index=word2ind['<pad>'])\\nfirst_model = WordLM(hidden_dim=128, vocab_size=len(vocab)).to(device)\\noptimizer = torch.optim.Adam(first_model.parameters())\\nlosses, perp = train(train_dataloader, eval_dataloader, first_model, criterion, optimizer, epochs=10)\""},"metadata":{}}]},{"cell_type":"code","source":"#plot_results(losses, perp)\n#print(perp)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:14:37.955165Z","iopub.execute_input":"2023-06-06T14:14:37.955699Z","iopub.status.idle":"2023-06-06T14:14:37.962541Z","shell.execute_reply.started":"2023-06-06T14:14:37.955663Z","shell.execute_reply":"2023-06-06T14:14:37.961713Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"def generate(model, input_seq:str, length=30) -> str:\n    device = 'cpu'\n    model = model.to(device)\n    input_ids = [word2ind['<bos>']] + [\n        word2ind.get(word, word2ind['<unk>']) for word in word_tokenize(input_seq)]\n    input_ids = torch.LongTensor(input_ids).to(device)\n    \n    model.eval()\n    with torch.no_grad():\n        for i in range(length):\n            next_word_distribution = model(input_ids)[-1]\n            # print(next_word_distribution.squeeze())\n            next_word = next_word_distribution.squeeze().argmax()\n            if next_word.item() == word2ind['<eos>']:\n                break\n            input_ids = torch.cat([input_ids, next_word.unsqueeze(0)])\n    \n    words = ' '.join([ind2word[idx.item()] for idx in input_ids])\n\n    return words","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:14:37.964016Z","iopub.execute_input":"2023-06-06T14:14:37.964356Z","iopub.status.idle":"2023-06-06T14:14:37.973872Z","shell.execute_reply.started":"2023-06-06T14:14:37.964327Z","shell.execute_reply":"2023-06-06T14:14:37.972838Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"#print(generate(first_model, input_seq='acting and directing'))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:14:37.975406Z","iopub.execute_input":"2023-06-06T14:14:37.975791Z","iopub.status.idle":"2023-06-06T14:14:37.987395Z","shell.execute_reply.started":"2023-06-06T14:14:37.975756Z","shell.execute_reply":"2023-06-06T14:14:37.986464Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"## 6. Эксперименты.\nСтруктура экспериментов предполагается следующей:\n```\n[EXP-1] - [Architecture]\n\t\t\t-[LSTM:RNN:GRU]\n            -[bidirectional]\n\t\t\t-[num_layers 1:5:10]\n[EXP-2] - [Dropout 0.1:0.5]\t\n[EXP-3] - [Batchnorm]\n[EXP-4] - [Hyperparams]\n---\n[EXP-5] - final model, long train\n```\nТесты проводятся со сроком 7 эпох. Затем лучшая модель+параметры обучаются 20 эпох","metadata":{}},{"cell_type":"code","source":"def start(model_type, \n          num_layers, \n          hidden_dim, \n          drop_rate, \n          batches,\n          epochs):\n    criterion = nn.CrossEntropyLoss(ignore_index=word2ind['<pad>'])\n    model = WordLM(hidden_dim=hidden_dim, \n                         vocab_size=len(vocab), \n                         num_layers=num_layers, \n                         drop_rate=drop_rate,\n                         batches=batches,\n                         rnn_type=model_type).to(device)\n    optimizer = torch.optim.Adam(first_model.parameters())\n    return model, train(train_dataloader, eval_dataloader, model, criterion, optimizer, epochs)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:14:37.989083Z","iopub.execute_input":"2023-06-06T14:14:37.989736Z","iopub.status.idle":"2023-06-06T14:14:37.998981Z","shell.execute_reply.started":"2023-06-06T14:14:37.989703Z","shell.execute_reply":"2023-06-06T14:14:37.998047Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"mlflow.set_experiment(experiment_name=f\"LM: EXP-1 [Architecture].\")\nmodels = [\"rnn\", \"gru\", \"lstm\"]\nparams = {\n        \"num_layers\": 1,\n        \"epochs\": 7,\n        \"hidden_dim\": 128,\n        \"drop_rate\": 0.1,\n        \"batches\": False\n    }\nfor model_type in models:\n    with mlflow.start_run():\n        mlflow.log_params(params)\n        mlflow.log_param(\"rnn_type\", model_type)\n        model, losses, perps = start(model_type=model_type, **params)\n        print(f\"Plots for {model_type}\")\n        plot_results(losses, perps)\n        criterion = nn.CrossEntropyLoss(ignore_index=word2ind['<pad>'])\n        perplexity = evaluate(model, criterion, eval_dataloader)\n        mlflow.log_metric(\"perplexity\", perplexity)\n        print(f\"Final perplexity: {perplexity}\")\n        ","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:14:38.000305Z","iopub.execute_input":"2023-06-06T14:14:38.001233Z","iopub.status.idle":"2023-06-06T14:17:54.306335Z","shell.execute_reply.started":"2023-06-06T14:14:38.001176Z","shell.execute_reply":"2023-06-06T14:17:54.304586Z"},"trusted":true},"execution_count":55,"outputs":[{"output_type":"display_data","data":{"text/plain":"Training epoch 0::   0%|          | 0/1267 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c293879616364333bd95792954a08e43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch 1::   0%|          | 0/1267 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c09c5adc9224eb1ae4021d73c5958e7"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[55], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mlog_params(params)\n\u001b[1;32m     13\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mlog_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrnn_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_type)\n\u001b[0;32m---> 14\u001b[0m model, losses, perps \u001b[38;5;241m=\u001b[39m \u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlots for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m plot_results(losses, perps)\n","Cell \u001b[0;32mIn[54], line 15\u001b[0m, in \u001b[0;36mstart\u001b[0;34m(model_type, num_layers, hidden_dim, drop_rate, batches, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m WordLM(hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim, \n\u001b[1;32m      9\u001b[0m                      vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vocab), \n\u001b[1;32m     10\u001b[0m                      num_layers\u001b[38;5;241m=\u001b[39mnum_layers, \n\u001b[1;32m     11\u001b[0m                      drop_rate\u001b[38;5;241m=\u001b[39mdrop_rate,\n\u001b[1;32m     12\u001b[0m                      batches\u001b[38;5;241m=\u001b[39mbatches,\n\u001b[1;32m     13\u001b[0m                      rnn_type\u001b[38;5;241m=\u001b[39mmodel_type)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(first_model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[47], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, eval_loader, model, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     29\u001b[0m perplexities \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 31\u001b[0m     epoch_losses \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msum\u001b[39m(epoch_losses) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(epoch_losses))\n\u001b[1;32m     33\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28msum\u001b[39m(epoch_losses) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(epoch_losses))\n","Cell \u001b[0;32mIn[47], line 4\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(train_dataloader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m      2\u001b[0m epoch_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      5\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      6\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mflatten(start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, end_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[45], line 18\u001b[0m, in \u001b[0;36mWordDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Предложения приведены к нижнему регистру, \u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# но не очищены от стоп-слов и пунктуации.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     tokenized_sentence \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbos_id]\n\u001b[1;32m     17\u001b[0m     tokenized_sentence \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 18\u001b[0m         word2ind\u001b[38;5;241m.\u001b[39mget(word, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munk_id) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m         ] \n\u001b[1;32m     20\u001b[0m     tokenized_sentence \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meos_id]\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_sentence\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/tokenize/__init__.py:131\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m:type preserver_line: bool\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/tokenize/__init__.py:132\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m:type preserver_line: bool\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences\n\u001b[0;32m--> 132\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/tokenize/treebank.py:136\u001b[0m, in \u001b[0;36mTreebankWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    133\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(substitution, text)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS2:\n\u001b[0;32m--> 136\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mregexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m1 \u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m2 \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS3:\n\u001b[1;32m    138\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2 \u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n","File \u001b[0;32m/opt/conda/lib/python3.10/re.py:324\u001b[0m, in \u001b[0;36m_subx\u001b[0;34m(pattern, template)\u001b[0m\n\u001b[1;32m    321\u001b[0m     template \u001b[38;5;241m=\u001b[39m sre_parse\u001b[38;5;241m.\u001b[39mparse_template(template, pattern)\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sre_parse\u001b[38;5;241m.\u001b[39mexpand_template(template, match)\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_subx\u001b[39m(pattern, template):\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;66;03m# internal: Pattern.sub/subn implementation helper\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     template \u001b[38;5;241m=\u001b[39m _compile_repl(template, pattern)\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m template[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(template[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;66;03m# literal replacement\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}